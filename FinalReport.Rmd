---
title: 'Project 1 : Final Report'
author:
- Anand Balan (axb173130@utdallas.edu)
- Gunjan Munjal (gxm171430@utdallas.edu)
- Pavithra Kamalakannan Ushakannan (pxk170930@utdallas.edu)
date: "11/02/2018"
output:
  html_document: default
  pdf_document: default
---
<style>
body {
text-align: justify}
</style>
# Ethereum
At its simplest, Ethereum is an open software platform based on blockchain technology that enables developers to build and deploy decentralized applications. 
In the Ethereum blockchain, instead of mining for bitcoin, miners work to earn Ether, a type of crypto token that fuels the network. Beyond a tradeable cryptocurrency, Ether is also used by application developers to pay for transaction fees and services on the Ethereum network. 
There is a second type of token that is used to pay miners fees for including transactions in their block, it is called gas, and every smart contract execution requires a certain amount of gas to be sent along with it to entice miners to put it in the blockchain.

#ERC20
ERC stands for Ethereum Request for Comments. It an official protocol for proposing improvements to the Ethereum(ETH) network. '20' is the unique proposal ID number. It defines a set of rules which need to be met in order for a token to be accepted and called an 'ERC20 Token'. The standard rules apply to all ERC20 Tokens since these rules are required to interact with each other on the Ethereum network. 

#Primary Token
The primary token we have considered for the data analysis is the 10th largest token, Binance Coin Token. The Binance Coin (BNB) is the native coin of the Binance cryptocurrency trading platform. The coin can be used to pay trading fees, transaction fees and listing fees on the platform. Binance incentivizes payment of fees via BNB by providing a substantial discount. BNB runs natively on the Ethereum blockchain and follows the ERC20 token standard. 
The token was established with a total supply of 200 million. 
The BNB token itself has multiple forms of utility, essentially being the underlying gas that powers the Binance Ecosystem. The current most prominent use cases include using BNB to:

- Pay for trading fees on the exchange, obtaining the equivalent of a 50% discount on trades (during the first year).

- Monaco has included support for Binance's BNB token to its platform which includes the Monaco Visa Card and mobile app.

- Uplive platform supports BNB to buy virtual gifts.

# Preprocessing

The first step in the analysis of the binance coin was to clean the dataset i.e. removing the outliers from the dataset. Upon studying the binance coin on [etherscan.io](etherscan.io), the following information were collected -  

```{r,echo=FALSE, warning = FALSE}
total_supply <- 10^18 * 192443301
#print(paste("Total supply",head(total_supply)))
```
* The sub-units of a binance token is **$10^{18}$**.  
* The total supply of binance tokens are **192,443,301 BNB**.  
* Hence the total sub-units supply of the binance tokens are: `r total_supply`


#1. Data Description

The data file corresponding to Binance Coin, _networkbnbTX.txt_, consist of four columns:  

1. Seller User ID 
2. Buyer User ID
3. Unix Timestamp
4. Amount (in sub-units)

```{r, warning = FALSE}
dftoken <- read.table("networkbnbTX.txt", header = FALSE)
names(dftoken)<-c("SenderID","ReceiverID","TimeStamp","Amount")
print(head(dftoken,10))
```

The price file corresponding to Binance Coin, _bnb.txt_, consist of seven columns:  

1. Date	
2. Open	
3. High	
4. Low	
5. Close
6. Volume
7. Market Cap

```{r, echo=FALSE, warning = FALSE}
tokenPrices <- read.table("bnb.txt", header=TRUE, sep="\t")
names(tokenPrices) <- c("date", "open", "high", "low","close", "volume", "cap")
print(head(tokenPrices,10))
```

##1.1 Identifying Outliers

To identify and remove outliers, first, we define an outlier with respect to the problem and the underlying data set . For the given problem, an outlier is any transaction whose amount is greater than the total supply of sub-units.

```{r, warning = FALSE}

dftoken <- read.table("networkbnbTX.txt", header = FALSE)
names(dftoken)<-c("SellerID","ReceiverID","TimeStamp","Amount")
dftokenDF <- as.data.frame(dftoken)
outliers <- dftokenDF[dftokenDF$Amount > total_supply]
print(paste("Length of Outliers",length(outliers)))
```

Since there is no exorbitant transaction, it is safe to declare that the dataset **does not** contain any outlier.

##1.2 Distribution for selling token
The primary objective for first part of the project was to find the distribution that fits the relationship between frequency of a user selling and buying a token.


```{r, warning = FALSE}
#---------- Number of sales ---------
countOfSeller<-table(dftoken$SellerID) # gives the frequency at which users sells tokens
frequencyDataFrame<-as.data.frame(countOfSeller)
colnames(frequencyDataFrame)<-c("SellerID","Count")
countOfCount<-table(frequencyDataFrame$Count) # gives the number of users who have made 'x' sales.
frequencyDataFrame2<-as.data.frame(countOfCount)
colnames(frequencyDataFrame2)<-c("Token Count Sold","Number of Users")
print(head(frequencyDataFrame2,10))
meanOfDistribution <- mean(frequencyDataFrame2$`Number of Users`)
#print(paste("Mean of Distribution",meanOfDistribution))
varianceOfDistibution<-var(frequencyDataFrame2$`Number of Users`)
#print(paste("Variance of Distribution",varianceOfDistibution))
```

The mean frequency of number of sales `r meanOfDistribution` and variance of `r varianceOfDistibution`

```{r}
#-------- Plotting 100% of the data ----------
attach(frequencyDataFrame2)
plot(y=frequencyDataFrame2$`Number of Users`, 
     x=frequencyDataFrame2$`Token Count Sold`, 
     ylab="Number of Users", 
     xlab="No of times a token was sold",
     main="Number of times user sells token",
     type="o")
```


From the graph, and the data used by the graph, we see that 16575 users made single transaction. The next highest number of transactions are done by 1981 users. In general, more than 99% of the users have made less than 6 transactions each.  

The trend clearly shows that as the number of users increase, the number of transactions reduce drastically. The resulting graph is cramped on the lower end of the x-axis (number of users who made 'x' sales).  To get a clear picture of the crowded portion, and hence to make it easier to find the distribution, we trimmed 99% of the data, thereby eliminating visibly isolated data points.

```{r, warning = FALSE}
#Removing 99% of the data and plotting the trimmed data.
plot(y=frequencyDataFrame2$`Number of Users`, 
     x=frequencyDataFrame2$`Token Count Sold`,  
     ylab="Number of Users", 
     xlab="No of times a token was sold",
     main="Number of times user sells token",
     ylim = c(0,165.75),
     type="o")
```

After this, analysis was done to fit the resulting data in one of the known distributions such as Exponential, Poisson, Negative Binomial or Weibull distribution and determine which distribution fits the best.

```{r, warning = FALSE}
#install.packages("fitdistrplus")
library(fitdistrplus)
exp <- fitdistr(frequencyDataFrame2$`Number of Users`, densfun = "exponential")

poisson <- fitdistr(frequencyDataFrame2$`Number of Users`, densfun = "poisson")

weibull <- fitdistr(frequencyDataFrame2$`Number of Users`, densfun = "weibull")

negBin <- fitdistr(frequencyDataFrame2$`Number of Users`, densfun = "negative binomial")
```
```{r, echo=FALSE}
negBin
```


Based on analysis, number of times user sells a token follows a negative binomial distribution with mean of `r meanOfDistribution` and a variance of `r varianceOfDistibution`.

##1.3 Distribution of buying tokens

```{r, warning = FALSE}
#---------- Number of purchases ---------
countOfBuyer<-table(dftoken$ReceiverID)
freqDataFrame<-as.data.frame(countOfBuyer)
colnames(freqDataFrame)<-c("ReceiverID","Frequency")
countOfFrequency<-table(freqDataFrame$Frequency)
freqDataFrame2<-as.data.frame(countOfFrequency)
colnames(freqDataFrame2)<-c("Token Count Bought","Number of Users")
print(head(freqDataFrame2,10))
meanOfDistribution2 <- mean(freqDataFrame2$`Number of Users`)
#print(paste("Mean of Distribution",meanOfDistribution2))
varianceOfDistibution2<-var(freqDataFrame2$`Number of Users`)
#print(paste("Variance of Distribution",varianceOfDistibution2))
```

The mean frequency of number of sales `r meanOfDistribution2` and variance of `r varianceOfDistibution2`

```{r, warning = FALSE}
#---------Plotting 100% of the data---------
attach(freqDataFrame2)
plot(y=freqDataFrame2$`Number of Users`, 
     x=freqDataFrame2$`Token Count Bought`, 
     ylab="Number of Users", 
     xlab="No of times a token was bought",
     main="Number of times user buys token",
     type="o")
```


From the graph, and the data used by the graph, we see that 252994 users have made single purchase. The next higher frequency purchase - 2 - is done by 28353 users. 

The trend again shows that lesser number of users make higher number of purchases. The resulting graph is cramped on the lower end of the x-axis (number of users who made 'x' purchases). To get a clear picture of the crowded portion, and hence to easily find the distribution, we trimmmed 99% of the data, thereby eliminating visibly isolated data points.

```{r, warning = FALSE}
#Removing 99% of the data and plotting the trimmed data.
plot(type="l", 
     y=freqDataFrame2$`Number of Users`, 
     x=freqDataFrame2$`Token Count Bought`, 
     ylab="Number of Users", 
     xlab="No of times a token was bought",
     main="Number of times user buys token",
     ylim = c(0,2600))
```

The resulting data that we have must be analysed by fitting the data into the known distributions such as Exponential, Poisson, Negative Binomial and Weibull distributions to evaluate the distribution that fits accurately.

```{r, warning = FALSE}
#install.packages("fitdistrplus")
library(fitdistrplus)
exp <- fitdistr(freqDataFrame2$`Number of Users`, densfun = "exponential")
poisson <- fitdistr(freqDataFrame2$`Number of Users`, densfun = "poisson")
weibull <- fitdistr(freqDataFrame2$`Number of Users`, densfun = "weibull")
negBin <- fitdistr(freqDataFrame2$`Number of Users`, densfun = "negative binomial")
```
```{r, echo=FALSE}
negBin
```

Based on analysis, number of times user buys a token follows a negative binomial distribution with mean of `r meanOfDistribution2` and a variance of `r varianceOfDistibution2`.

#2. Layering of token transactions based on amount.

The second part of the project was to compute the correlation of price data with each of the layers (hint: start by looking at Pearson correlation).

Hint: How to create layers? This descriptive statistic is similar to bin selection in histograms. For example, we could choose $\layer_{1}$ as those transactions that involve $\ 0.01*max_{t}$. Find a good value for the number of layers and justify your choice. 

##2.1 Factors

We have chosen the following two factors:

1. Number of transactions
2. Price of transaction in USD

To make the file readable, we first cleaned both the data files and assigned suitable header to the columns.
```{r, warning = FALSE}

binanceTokenGraph <- read.table("networkbnbTX.txt", header = FALSE)
names(binanceTokenGraph)<-c("SellerID","ReceiverID","TimeStamp","Amount")

tokenPrices <- read.table("bnb.txt", header=TRUE, sep="\t")
names(tokenPrices) <- c("date", "open", "high", "low","close", "volume", "cap")
```

The next part of the process was to combine the two files, _networkbnbTX.txt_ and _bnb.txt_, on the date column. In _bnb.txt_, the date column was available but for _networkbnbTX.txt_, we converted the UNIX time stamps into dates.
This enabled us to combine the files on respective date columns.
```{r, echo=FALSE, warning = FALSE}
dftoken$TimeStamp <- format(as.Date(as.POSIXct(as.numeric(as.character(dftoken$TimeStamp)),origin="1970/01/01",tz="GMT"), tz="GMT"), "%m/%d/%Y")
```


```{r, warning = FALSE}
tokenPrices$volume <- as.numeric(gsub(",", "", tokenPrices$volume))
tokenPrices <- data.frame(tokenPrices$date, tokenPrices$close, tokenPrices$volume, tokenPrices$volume / tokenPrices$close )
names(tokenPrices)<-c("date","close", "volume","tokens_transacted")
```
The data was available for different range of dates so the merger was done by taking the intersection of the date column in the two given files.

```{r, warning = FALSE}
tokenPrices <- tokenPrices[tokenPrices$date %in% unique(dftoken$TimeStamp), ]
dftoken <- dftoken[dftoken$TimeStamp %in% tokenPrices$date, ]
```

We converted the subunits into tokens, and the number of tokens transacted on the dates available in the intersection region were added to _binanceTokenGraph_ as a new column _AmountInTokens_, sorted by chronological ordering.
```{r, warning = FALSE}
dftoken$AmountInTokens <- (dftoken$Amount / (10^18)) 
```


```{r, warning = FALSE}
tokenPrices <- tokenPrices[with(tokenPrices, order(date)),]
dftoken <- dftoken[with(dftoken, order(TimeStamp)),]
```

To calculate the number of transactions in respective layers and the cumulative amount of transactions (in USD) in each layer, we follow the algorithm states below-

1. Filter all transactions for the layer.
2. Calculate the total number of tokens transacted for each day in the layer.
3. Multiply the token count per day with the closing price of the same day from _tokenPrices_ data.
4. Add the tokenPrices of every day to get the total amount (in USD) of the transactions in that layer.

```{r, warning = FALSE}
maxAmount <- max(dftoken$AmountInTokens)
i <- 1
maxCorr <- -2
no_of_txn_in_layer <- c()
total_amount_trans_in_layer <- c()

repeat {
  threshold <- as.integer(((i/31) * maxAmount))
  transactions_in_layer <- dftoken[dftoken$AmountInTokens >= threshold, ]
  no_of_records <- nrow(transactions_in_layer)
  if (no_of_records == 0) {
    break
  }
  no_of_transactions_per_day <- aggregate(AmountInTokens ~ TimeStamp, transactions_in_layer, length)
  total_amount_trans_per_day <- aggregate(AmountInTokens ~ TimeStamp, transactions_in_layer, sum)
  names(no_of_transactions_per_day) <- c("date", "no_of_transactions")
  names(total_amount_trans_per_day) <- c("date", "total_amount_per_day")
  
  no_of_txn_in_layer <- c(no_of_txn_in_layer, no_of_records)
  amnt_txn_in_layer <- total_amount_trans_per_day$total_amount_per_day * tokenPrices[tokenPrices$date %in% unique(total_amount_trans_per_day$date), ]$close
  total_amount_trans_in_layer <- c(total_amount_trans_in_layer, sum(amnt_txn_in_layer))
  i <- i + 1
}
```
We calculated the correlation values for a set of numbers and narrowed down to 31 as it gives the highest correlation value.
```{r, warning = FALSE}
library(knitr)
library(kableExtra)
dt <- read.table("corrValue.txt", header = TRUE)
dt %>%
  kable() %>%
  kable_styling()
```

```{r}
corr_df <- data.frame(no_of_txn=no_of_txn_in_layer, amount_in_usd_transacted=total_amount_trans_in_layer)
```
#2.2 Correlation Dataframe
```{r, warning = FALSE}
print(corr_df)
#print(paste("Correlation Value", cor(corr_df, use="complete.obs", method="pearson")[2]))
cor_value <- cor(corr_df, use="complete.obs", method="pearson")
library(ggplot2)
ggplot(corr_df, aes(x=no_of_txn, y=amount_in_usd_transacted, group=1)) + geom_line()
```
The correlation value is : `r cor_value`
#3. Analysing behaviour of active user
This part of the project  was dedicated towards finding how the most active users in one token behave in other token networks and fit a distribution for the number of unique tokens they invest in.

##3.1 Most Active Users in Primary Token 
The goal of this step is to find the most active buyers and sellers of the primary token - _networkbnbTX.txt_. The most active buyers and sellers are found on the basis of the number of transactions that they have done.

```{r, warning = FALSE}
# read our primary token...
tokenGraph <- read.table("networkbnbTX.txt", header = FALSE)
names(tokenGraph)<-c("SellerID","ReceiverID","TimeStamp","Amount")

# get the top ten active sellers in primary token...
sellerFreq <- data.frame(table(tokenGraph$SellerID))
names(sellerFreq) <- c("seller", "txn_count")
sellerFreq <- sellerFreq[order(-sellerFreq$txn_count), ]
sellerFreq <- sellerFreq[1:10,]

# get the top ten active buyers in primary token...
buyerFreq <- data.frame(table(tokenGraph$ReceiverID))
names(buyerFreq) <- c("buyer", "txn_count")
buyerFreq <- buyerFreq[order(-buyerFreq$txn_count), ]
buyerFreq <- buyerFreq[1:10,]
```

Now, we trimmed our data to the top 10 sellers and top 10 buyers from the primary token. The next step is to merge these users into a single vector and find the unique users among them. This is because, it is possible for one user to be the most active seller and buyer. For example, an user who performs algorithmic trading on the token will have frequent sales and purchases.

```{r, warning = FALSE}
# get the top ten active sellers in primary token...
topUsers <- c(as.vector(sellerFreq$seller), as.vector(buyerFreq$buyer))
topUsers <- unique(as.numeric(topUsers))
print(topUsers)
```

#3.2 Finding Behavior of Active Users of Primary Token

The second step of the process was to find the activity of the active users found above in other token networks. For this, we go through every other token network and look for the above active users in those networks. If any of those users have bought or sold a token in the other network, we increment the corresponding counter for that user.

We consider both purchase and sale for tracking the activity because -

1. A sale of token indicates that the user previously invested in the token.
2. A purchase of token indicates that the user is now investing in the token.

```{r, warning = FALSE}
result <- data.frame(user_id=topUsers)
result$uniqTokens <- 0

# my data files (all token graphs) are stored at ./data/tokenGraphs where . is the place where this R file resides.
tokenGraphFiles <- list.files(paste(getwd(),"data","tokenGraphs",sep="/"))

# iterating over all the tokenGraphs except the primary token.
for (tokenGraphFile in tokenGraphFiles) {
  if (tokenGraphFile != "networkbnbTX.txt") {
    # read the token graph into the form we need...
    tempTokenGraph <- read.table(paste("data", "tokenGraphs", tokenGraphFile, sep="/"), header = FALSE)
    names(tempTokenGraph)<-c("SellerID","ReceiverID","TimeStamp","Amount")
    
    # find the number of times every seller has made a sale of the token in this iteration...
    tempSellerFreq <- data.frame(table(tempTokenGraph$SellerID))
    names(tempSellerFreq) <- c("user_id", "txn_count")
    
    # find the number of times every buyer has made a purchase of the token in this iteration...
    tempBuyerFreq <- data.frame(table(tempTokenGraph$ReceiverID))
    names(tempBuyerFreq) <- c("user_id", "txn_count")
    
    # filter only the sellers using our list of most active sellers/buyers in our primary token...
    tempSellerFreq <- tempSellerFreq[tempSellerFreq$user_id %in% topUsers, ]
    # filter only the buyers using our list of most active sellers/buyers in our primary token...
    tempBuyerFreq <- tempBuyerFreq[tempBuyerFreq$user_id %in% topUsers, ]
    
    # append the buyers of this iteration's token to the sellers of this iteration's token...
    txnInAnotherToken <- rbind(tempSellerFreq, tempBuyerFreq)
    # if there are any user in the iteration's token matching with any of our most active users...
    if (nrow(txnInAnotherToken) > 0) {
      # add one to the "result" map for each user in the filtered list "txnInAnotherToken"...
      # (the below code does just that work even though it appears complicated...)
      tempDist <- data.frame(user_id=unique(as.numeric(as.vector(txnInAnotherToken$user_id))), uniqTokens=1)
      result <- rbind(result, tempDist)
      result <- aggregate(uniqTokens ~ user_id, result, sum)
    }
  }
}
```

Now, the variable result contains user id vs number of tokens he/she invested in apart from the primary token. The next step is to calculate the number of users who have invested in 'x' number of unique tokens. i.e. we need a new table that has the number of unique tokens users have invested in as one column and the number of users who have invested in that many tokens as another column.

```{r, warning = FALSE}
result <- data.frame(table(result$uniqTokens))
names(result) <- c('uniqueTokenCount', 'usersCount')
```

Please note, that the new table will not contain records for the full range of values for number of unique tokens. We have 38 token networks apart from the primary token. Therefore, we should have 38 records in the new table. To fill the remaining records with default number of users, we do the following -

```{r, warning = FALSE}
tempDist <- data.frame(uniqueTokenCount=seq(0,length(tokenGraphFiles)-1), usersCount=1)
result <- rbind(tempDist, result)
result <- aggregate(usersCount ~ uniqueTokenCount, result, sum)
result <- result[order(as.numeric(result$uniqueTokenCount)),]
result$uniqueTokenCount <- as.numeric(result$uniqueTokenCount)
```

Now, in result we have  <br />
P(#UniqueTokens = 0) = Number of users who invested in 0 tokens. <br />
P(#UniqueTokens = 1) = Number of users who invested in 1 token. <br />
P(#UniqueTokens = 2) = Number of users who invested in 2 tokens. <br />
... <br />
P(#UniqueTokens = 38) = Number of users who invested in 38 tokens. <br />

##3.3 Fitting the distribution of the number of unique tokens users have invested in

We plot the distribution as follows -
```{r}
library('ggplot2')
print(ggplot(result, aes(x=uniqueTokenCount, y=usersCount, group=1)) + geom_line())
```

The mean of the data our data is 
```{r, warning = FALSE}
print(mean(result$usersCount))
```

From the plot, we can see the distribution is close to Log Normal or Weibull or Exponential. We then try to fit the data with them.

```{r, warning = FALSE}
# install.packages('fitdistrplus', repos = "http://cran.us.r-project.org")
library('fitdistrplus')
lnorm <- fitdist(result$usersCount, "lnorm")
plot(lnorm)
summary(lnorm)

weibull <- fitdist(result$usersCount, "weibull")
plot(weibull)
summary(weibull)

exp <- fitdist(result$usersCount, "exp")
plot(exp)
summary(exp)

par(mfrow=c(2,2))
plot.legend <- c("Weibull", "lognormal", "exp")
denscomp(list(weibull, lnorm, exp ), legendtext = plot.legend)
cdfcomp (list(weibull, lnorm, exp ), legendtext = plot.legend)
qqcomp  (list(weibull, lnorm, exp ), legendtext = plot.legend)
ppcomp  (list(weibull, lnorm, exp ), legendtext = plot.legend)
```


#4. Conclusion:
This is the summarization for each part of the process.

Number of times user sells a token follows a negative binomial distribution with mean of `r meanOfDistribution` and a variance of `r varianceOfDistibution`.

Number of times user buys a token follows a negative binomial distribution with mean of `r meanOfDistribution2` and a variance of `r varianceOfDistibution2`.

From the graph it can be inferred that as the number of transactions in a layer increases, the total amount transacted increases. Since the total amount(in USD) is calculated using the closing price of the days the transactions were made, we can conclude that the number of transactions in each layer is positively correlated to the closing price.

From the plots and the estimated parameters, we found that **Weibull distribution** closely fits our data. Hence, the distribution of the number of unique tokens that users have invested in (apart from our primary token) follows a Weibull distribution.
