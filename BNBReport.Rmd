---
title: 'Final Project Report'
author:
- Anand Balan (axb173130@utdallas.edu)
- Gunjan Munjal (gxm171430@utdallas.edu)
- Pavithra Kamalakannan Ushakannan (pxk170930@utdallas.edu)
date: "12/01/2018"
output:
  html_document: default
---
<style>
body {
text-align: justify}
</style>
# Ethereum
At its simplest, Ethereum is an open software platform based on blockchain technology that enables developers to build and deploy decentralized applications. 
In the Ethereum blockchain, instead of mining for bitcoin, miners work to earn Ether, a type of crypto token that fuels the network. Beyond a tradeable cryptocurrency, Ether is also used by application developers to pay for transaction fees and services on the Ethereum network. 
There is a second type of token that is used to pay miners fees for including transactions in their block, it is called gas, and every smart contract execution requires a certain amount of gas to be sent along with it to entice miners to put it in the blockchain.

#ERC20
ERC stands for Ethereum Request for Comments. It is an official protocol for proposing improvements to the Ethereum(ETH) network. '20' is the unique proposal ID number. It defines a set of rules which need to be met in order for a token to be accepted and called an 'ERC20 Token'. The standard rules apply to all ERC20 Tokens since these rules are required to interact with each other on the Ethereum network. 

#Primary Token
The primary token we have considered for the data analysis is the Binance Coin Token. The Binance Coin (BNB) is the native coin of the Binance cryptocurrency trading platform. The coin can be used to pay trading fees, transaction fees and listing fees on the platform. Binance incentivizes payment of fees via BNB by providing a substantial discount. BNB runs natively on the Ethereum blockchain and follows the ERC20 token standard. 
The token was established with a total supply of 200 million. 
The BNB token itself has multiple forms of utility, essentially being the underlying gas that powers the Binance Ecosystem. The current most prominent use cases include using BNB to:

- Pay for trading fees on the exchange, obtaining the equivalent of a 50% discount on trades (during the first year).

- Monaco has included support for Binance's BNB token to its platform which includes the Monaco Visa Card and mobile app.

- Uplive platform supports BNB to buy virtual gifts.

# Preprocessing

The first step in the analysis of the binance coin was to clean the dataset i.e. removing the outliers from the dataset. Upon studying the binance coin on [etherscan.io](etherscan.io), the following information were collected -  

```{r,echo=FALSE, warning = FALSE}
total_supply <- 10^18 * 192443301
#print(paste("Total supply",head(total_supply)))
```
* The sub-units of a binance token is **$10^{18}$**.  
* The total supply of binance tokens are **192,443,301 BNB**.  
* Hence the total sub-units supply of the binance tokens are: `r total_supply`


#1. Data Description

The data file corresponding to Binance Coin, _networkbnbTX.txt_, consist of four columns:  

1. Seller User ID 
2. Buyer User ID
3. Unix date
4. amount (in sub-units)

```{r, warning = FALSE}
binanceTokenGraph <- read.table("networkbnbTX.txt", header = FALSE)
names(binanceTokenGraph)<-c("seller","buyer","date","amount")
print(head(binanceTokenGraph,10))
```

The price file corresponding to Binance Coin, _bnb.txt_, consist of seven columns:  

1. Date	
2. Open	
3. High	
4. Low	
5. Close
6. Volume
7. Market Cap

```{r, echo=FALSE, warning = FALSE}
tokenPrices <- read.table("bnb.txt", header=TRUE, sep="\t")
names(tokenPrices) <- c("date", "open", "high", "low","close", "volume", "cap")
print(head(tokenPrices,10))
```

##1.1 Identifying Outliers

To identify and remove outliers, first, we define an outlier with respect to the problem and the underlying data set . For the given problem, an outlier is any transaction whose amount is greater than the total supply of sub-units.

```{r, warning = FALSE}

binanceTokenGraph <- read.table("networkbnbTX.txt", header = FALSE)
names(binanceTokenGraph)<-c("SellerID","buyer","date","amount")
binanceTokenGraphDF <- as.data.frame(binanceTokenGraph)
outliers <- binanceTokenGraphDF[binanceTokenGraphDF$amount > total_supply]
print(paste("Length of Outliers",length(outliers)))
```

Since there is no exorbitant transaction, it is safe to declare that the dataset **does not** contain any outlier.

##1.2 Distribution for selling token
The primary objective for first part of the project was to find the distribution that fits the relationship between frequency of a user selling and buying a token.


```{r, warning = FALSE}
#---------- Number of sales ---------
countOfSeller<-table(binanceTokenGraph$SellerID) # gives the frequency at which users sells tokens
frequencyDataFrame<-as.data.frame(countOfSeller)
colnames(frequencyDataFrame)<-c("SellerID","Count")
countOfCount<-table(frequencyDataFrame$Count) # gives the number of users who have made 'x' sales.
frequencyDataFrame2<-as.data.frame(countOfCount)
colnames(frequencyDataFrame2)<-c("Token Count Sold","Number of Users")
print(head(frequencyDataFrame2,10))
meanOfDistribution <- mean(frequencyDataFrame2$`Number of Users`)
#print(paste("Mean of Distribution",meanOfDistribution))
varianceOfDistibution<-var(frequencyDataFrame2$`Number of Users`)
#print(paste("Variance of Distribution",varianceOfDistibution))
```

The mean frequency of number of sales **`r meanOfDistribution`** and variance of **`r varianceOfDistibution`**.

```{r}
#-------- Plotting 100% of the data ----------
attach(frequencyDataFrame2)
plot(y=frequencyDataFrame2$`Number of Users`, 
     x=frequencyDataFrame2$`Token Count Sold`, 
     ylab="Number of Users", 
     xlab="No of times a token was sold",
     main="Number of times user sells token",
     type="o")
```


From the graph, and the data used by the graph, we see that 16575 users made single transaction. The next highest number of transactions are done by 1981 users. In general, more than 99% of the users have made less than 6 transactions each.  

The trend clearly shows that as the number of users increase, the number of transactions reduce drastically. The resulting graph is cramped on the lower end of the x-axis (number of users who made 'x' sales).  To get a clear picture of the crowded portion, and hence to make it easier to find the distribution, we trimmed 99% of the data, thereby eliminating visibly isolated data points.

```{r, warning = FALSE}
#Removing 99% of the data and plotting the trimmed data.
plot(y=frequencyDataFrame2$`Number of Users`, 
     x=frequencyDataFrame2$`Token Count Sold`,  
     ylab="Number of Users", 
     xlab="No of times a token was sold",
     main="Number of times user sells token",
     ylim = c(0,165.75),
     type="o")
```

After this, analysis was done to fit the resulting data in one of the known distributions such as Exponential, Poisson, Negative Binomial or Weibull distribution and determine which distribution fits the best.

```{r, warning = FALSE}
#install.packages("fitdistrplus")
library(fitdistrplus)
exp <- fitdistr(frequencyDataFrame2$`Number of Users`, densfun = "exponential")

poisson <- fitdistr(frequencyDataFrame2$`Number of Users`, densfun = "poisson")

weibull <- fitdistr(frequencyDataFrame2$`Number of Users`, densfun = "weibull")

negBin <- fitdistr(frequencyDataFrame2$`Number of Users`, densfun = "negative binomial")
```
```{r, echo=FALSE}
negBin
```


Based on analysis, number of times user sells a token follows a negative binomial distribution with mean of **`r meanOfDistribution`** and a variance of **`r varianceOfDistibution`**.

##1.3 Distribution of buying tokens

```{r, warning = FALSE}
#---------- Number of purchases ---------
countOfBuyer<-table(binanceTokenGraph$buyer)
freqDataFrame<-as.data.frame(countOfBuyer)
colnames(freqDataFrame)<-c("buyer","Frequency")
countOfFrequency<-table(freqDataFrame$Frequency)
freqDataFrame2<-as.data.frame(countOfFrequency)
colnames(freqDataFrame2)<-c("Token Count Bought","Number of Users")
print(head(freqDataFrame2,10))
meanOfDistribution2 <- mean(freqDataFrame2$`Number of Users`)
#print(paste("Mean of Distribution",meanOfDistribution2))
varianceOfDistibution2<-var(freqDataFrame2$`Number of Users`)
#print(paste("Variance of Distribution",varianceOfDistibution2))
```

The mean frequency of number of sales **`r meanOfDistribution2`** and variance of **`r varianceOfDistibution2`**.

```{r, warning = FALSE}
#---------Plotting 100% of the data---------
attach(freqDataFrame2)
plot(y=freqDataFrame2$`Number of Users`, 
     x=freqDataFrame2$`Token Count Bought`, 
     ylab="Number of Users", 
     xlab="No of times a token was bought",
     main="Number of times user buys token",
     type="o")
```


From the graph, and the data used by the graph, we see that 252994 users have made single purchase. The next higher frequency purchase - 2 - is done by 28353 users. 

The trend again shows that lesser number of users make higher number of purchases. The resulting graph is cramped on the lower end of the x-axis (number of users who made 'x' purchases). To get a clear picture of the crowded portion, and hence to easily find the distribution, we trimmmed 99% of the data, thereby eliminating visibly isolated data points.

```{r, warning = FALSE}
#Removing 99% of the data and plotting the trimmed data.
plot(type="l", 
     y=freqDataFrame2$`Number of Users`, 
     x=freqDataFrame2$`Token Count Bought`, 
     ylab="Number of Users", 
     xlab="No of times a token was bought",
     main="Number of times user buys token",
     ylim = c(0,2600))
```

The resulting data that we have must be analysed by fitting the data into the known distributions such as Exponential, Poisson, Negative Binomial and Weibull distributions to evaluate the distribution that fits accurately.

```{r, warning = FALSE}
#install.packages("fitdistrplus")
library(fitdistrplus)
exp <- fitdistr(freqDataFrame2$`Number of Users`, densfun = "exponential")
poisson <- fitdistr(freqDataFrame2$`Number of Users`, densfun = "poisson")
weibull <- fitdistr(freqDataFrame2$`Number of Users`, densfun = "weibull")
negBin <- fitdistr(freqDataFrame2$`Number of Users`, densfun = "negative binomial")
```
```{r, echo=FALSE}
negBin
```

Based on analysis, number of times user buys a token follows a negative binomial distribution with mean of **`r meanOfDistribution2`** and a variance of **`r varianceOfDistibution2`**.

#2. Layering of token transactions based on amount.

The second part of the project was to compute the correlation of price data with each of the layers (hint: start by looking at Pearson correlation).

Hint: How to create layers? This descriptive statistic is similar to bin selection in histograms. For example, we could choose $\layer_{1}$ as those transactions that involve $\ 0.01*max_{t}$. Find a good value for the number of layers and justify your choice. 

##2.1 Factors

We have chosen the following two factors:

1. Number of transactions
2. Price of transaction in USD

To make the file readable, we first cleaned both the data files and assigned suitable header to the columns.
```{r, warning = FALSE}

binanceTokenGraph <- read.table("networkbnbTX.txt", header = FALSE)
names(binanceTokenGraph)<-c("SellerID","buyer","date","amount")

tokenPrices <- read.table("bnb.txt", header=TRUE, sep="\t")
names(tokenPrices) <- c("date", "open", "high", "low","close", "volume", "cap")
```

The next part of the process was to combine the two files, _networkbnbTX.txt_ and _bnb.txt_, on the date column. In _bnb.txt_, the date column was available but for _networkbnbTX.txt_, we converted the UNIX time stamps into dates.
This enabled us to combine the files on respective date columns.
```{r, echo=FALSE, warning = FALSE}
binanceTokenGraph$date <- format(as.Date(as.POSIXct(as.numeric(as.character(binanceTokenGraph$date)),origin="1970/01/01",tz="GMT"), tz="GMT"), "%m/%d/%Y")
```


```{r, warning = FALSE}
tokenPrices$volume <- as.numeric(gsub(",", "", tokenPrices$volume))
tokenPrices <- data.frame(tokenPrices$date, tokenPrices$close, tokenPrices$volume, tokenPrices$volume / tokenPrices$close )
names(tokenPrices)<-c("date","close", "volume","tokens_transacted")
```
The data was available for different range of dates so a merger was done by taking the intersection of the date column in the two given files.

```{r, warning = FALSE}
tokenPrices <- tokenPrices[tokenPrices$date %in% unique(binanceTokenGraph$date), ]
binanceTokenGraph <- binanceTokenGraph[binanceTokenGraph$date %in% tokenPrices$date, ]
```

We converted the subunits into tokens, and the number of tokens transacted on the dates available in the intersection region were added to _binanceTokenGraph_ as a new column _amountInTokens_, sorted by chronological ordering.
```{r, warning = FALSE}
binanceTokenGraph$amountInTokens <- (binanceTokenGraph$amount / (10^18)) 
```


```{r, warning = FALSE}
tokenPrices <- tokenPrices[with(tokenPrices, order(date)),]
binanceTokenGraph <- binanceTokenGraph[with(binanceTokenGraph, order(date)),]
```

To calculate the number of transactions in respective layers and the cumulative amount of transactions (in USD) in each layer, we follow the algorithm stated below-

1. Filter all transactions for the layer.
2. Calculate the total number of tokens transacted for each day in the layer.
3. Multiply the token count per day with the closing price of the same day from _tokenPrices_ data.
4. Add the tokenPrices of every day to get the total amount (in USD) of the transactions in that layer.

```{r, warning = FALSE}
maxamount <- max(binanceTokenGraph$amountInTokens)
i <- 1
maxCorr <- -2
no_of_txn_in_layer <- c()
total_amount_trans_in_layer <- c()

repeat {
  threshold <- as.integer(((i/31) * maxamount))
  transactions_in_layer <- binanceTokenGraph[binanceTokenGraph$amountInTokens >= threshold, ]
  no_of_records <- nrow(transactions_in_layer)
  if (no_of_records == 0) {
    break
  }
  no_of_transactions_per_day <- aggregate(amountInTokens ~ date, transactions_in_layer, length)
  total_amount_trans_per_day <- aggregate(amountInTokens ~ date, transactions_in_layer, sum)
  names(no_of_transactions_per_day) <- c("date", "no_of_transactions")
  names(total_amount_trans_per_day) <- c("date", "total_amount_per_day")
  
  no_of_txn_in_layer <- c(no_of_txn_in_layer, no_of_records)
  amnt_txn_in_layer <- total_amount_trans_per_day$total_amount_per_day * tokenPrices[tokenPrices$date %in% unique(total_amount_trans_per_day$date), ]$close
  total_amount_trans_in_layer <- c(total_amount_trans_in_layer, sum(amnt_txn_in_layer))
  i <- i + 1
}
```
We calculated the **correlation values** for a set of numbers and **narrowed down to 31 as it gives the highest correlation value**.
```{r, warning = FALSE}
library(knitr)
library(kableExtra)
dt <- read.table("corrValue.txt", header = TRUE)
dt %>%
  kable() %>%
  kable_styling()
```

```{r}
corr_df <- data.frame(no_of_txn=no_of_txn_in_layer, amount_in_usd_transacted=total_amount_trans_in_layer)
```
#2.2 Correlation Dataframe
```{r, warning = FALSE}
print(corr_df)
#print(paste("Correlation Value", cor(corr_df, use="complete.obs", method="pearson")[2]))
cor_value <- cor(corr_df, use="complete.obs", method="pearson")
library(ggplot2)
ggplot(corr_df, aes(x=no_of_txn, y=amount_in_usd_transacted, group=1)) + geom_line()
```

**The correlation value is : `r cor_value[2]`**

#3. Analysing behaviour of active user
This part of the project  was dedicated towards finding how the most active users in one token behave in other token networks and fit a distribution for the number of unique tokens they invest in.

##3.1 Most Active Users in Primary Token 
The goal of this step is to find the most active buyers and sellers of the primary token - _networkbnbTX.txt_. The most active buyers and sellers are found on the basis of the number of transactions that they have done.

```{r, warning = FALSE}
# read our primary token...
tokenGraph <- read.table("networkbnbTX.txt", header = FALSE)
names(tokenGraph)<-c("SellerID","buyer","date","amount")

# get the top ten active sellers in primary token...
sellerFreq <- data.frame(table(tokenGraph$SellerID))
names(sellerFreq) <- c("seller", "txn_count")
sellerFreq <- sellerFreq[order(-sellerFreq$txn_count), ]
sellerFreq <- sellerFreq[1:10,]

# get the top ten active buyers in primary token...
buyerFreq <- data.frame(table(tokenGraph$buyer))
names(buyerFreq) <- c("buyer", "txn_count")
buyerFreq <- buyerFreq[order(-buyerFreq$txn_count), ]
buyerFreq <- buyerFreq[1:10,]
```

Now, we trimmed our data to the top 10 sellers and top 10 buyers from the primary token. The next step is to merge these users into a single vector and find the unique users among them. This is because, it is possible for one user to be the most active seller and buyer. For example, an user who performs algorithmic trading on the token will have frequent sales and purchases.

```{r, warning = FALSE}
# get the top ten active sellers in primary token...
topUsers <- c(as.vector(sellerFreq$seller), as.vector(buyerFreq$buyer))
topUsers <- unique(as.numeric(topUsers))
print(topUsers)
```

#3.2 Finding Behavior of Active Users of Primary Token

The second step of the process was to find the activity of the active users found above in other token networks. For this, we go through every other token network and look for the above active users in those networks. If any of those users have bought or sold a token in the other network, we increment the corresponding counter for that user.

We consider both purchase and sale for tracking the activity because -

1. A sale of token indicates that the user previously invested in the token.
2. A purchase of token indicates that the user is now investing in the token.

```{r, warning = FALSE}
result <- data.frame(user_id=topUsers)
result$uniqTokens <- 0

# my data files (all token graphs) are stored at ./data/tokenGraphs where . is the place where this R file resides.
tokenGraphFiles <- list.files(paste(getwd(),"data","tokenGraphs",sep="/"))

# iterating over all the tokenGraphs except the primary token.
for (tokenGraphFile in tokenGraphFiles) {
  if (tokenGraphFile != "networkbnbTX.txt") {
    # read the token graph into the form we need...
    tempTokenGraph <- read.table(paste("data", "tokenGraphs", tokenGraphFile, sep="/"), header = FALSE)
    names(tempTokenGraph)<-c("SellerID","buyer","date","amount")
    
    # find the number of times every seller has made a sale of the token in this iteration...
    tempSellerFreq <- data.frame(table(tempTokenGraph$SellerID))
    names(tempSellerFreq) <- c("user_id", "txn_count")
    
    # find the number of times every buyer has made a purchase of the token in this iteration...
    tempBuyerFreq <- data.frame(table(tempTokenGraph$buyer))
    names(tempBuyerFreq) <- c("user_id", "txn_count")
    
    # filter only the sellers using our list of most active sellers/buyers in our primary token...
    tempSellerFreq <- tempSellerFreq[tempSellerFreq$user_id %in% topUsers, ]
    # filter only the buyers using our list of most active sellers/buyers in our primary token...
    tempBuyerFreq <- tempBuyerFreq[tempBuyerFreq$user_id %in% topUsers, ]
    
    # append the buyers of this iteration's token to the sellers of this iteration's token...
    txnInAnotherToken <- rbind(tempSellerFreq, tempBuyerFreq)
    # if there are any user in the iteration's token matching with any of our most active users...
    if (nrow(txnInAnotherToken) > 0) {
      # add one to the "result" map for each user in the filtered list "txnInAnotherToken"...
      # (the below code does just that work even though it appears complicated...)
      tempDist <- data.frame(user_id=unique(as.numeric(as.vector(txnInAnotherToken$user_id))), uniqTokens=1)
      result <- rbind(result, tempDist)
      result <- aggregate(uniqTokens ~ user_id, result, sum)
    }
  }
}
```

Now, the variable result contains user id vs number of tokens he/she invested in apart from the primary token. The next step is to calculate the number of users who have invested in 'x' number of unique tokens. i.e. we need a new table that has the number of unique tokens users have invested in as one column and the number of users who have invested in that many tokens as another column.

```{r, warning = FALSE}
result <- data.frame(table(result$uniqTokens))
names(result) <- c('uniqueTokenCount', 'usersCount')
```

Please note, that the new table will not contain records for the full range of values for number of unique tokens. We have 38 token networks apart from the primary token. Therefore, we should have 38 records in the new table. To fill the remaining records with default number of users, we do the following -

```{r, warning = FALSE}
tempDist <- data.frame(uniqueTokenCount=seq(0,length(tokenGraphFiles)-1), usersCount=1)
result <- rbind(tempDist, result)
result <- aggregate(usersCount ~ uniqueTokenCount, result, sum)
result <- result[order(as.numeric(result$uniqueTokenCount)),]
result$uniqueTokenCount <- as.numeric(result$uniqueTokenCount)
```

Now, in result we have  <br />
P(#UniqueTokens = 0) = Number of users who invested in 0 tokens. <br />
P(#UniqueTokens = 1) = Number of users who invested in 1 token. <br />
P(#UniqueTokens = 2) = Number of users who invested in 2 tokens. <br />
... <br />
P(#UniqueTokens = 38) = Number of users who invested in 38 tokens. <br />

##3.3 Fitting the distribution of the number of unique tokens users have invested in

We plot the distribution as follows -
```{r}
library('ggplot2')
print(ggplot(result, aes(x=uniqueTokenCount, y=usersCount, group=1)) + geom_line())
```

The mean of the data our data is 
```{r, warning = FALSE}
print(mean(result$usersCount))
```

From the plot, we can see the distribution is close to Log Normal or Weibull or Exponential. We then try to fit the data with them.

```{r, warning = FALSE}
# install.packages('fitdistrplus', repos = "http://cran.us.r-project.org")
library('fitdistrplus')
lnorm <- fitdist(result$usersCount, "lnorm")
plot(lnorm)
summary(lnorm)

weibull <- fitdist(result$usersCount, "weibull")
plot(weibull)
summary(weibull)

exp <- fitdist(result$usersCount, "exp")
plot(exp)
summary(exp)

par(mfrow=c(2,2))
plot.legend <- c("Weibull", "lognormal", "exp")
denscomp(list(weibull, lnorm, exp ), legendtext = plot.legend)
cdfcomp (list(weibull, lnorm, exp ), legendtext = plot.legend)
qqcomp  (list(weibull, lnorm, exp ), legendtext = plot.legend)
ppcomp  (list(weibull, lnorm, exp ), legendtext = plot.legend)
```

#4. Predicting the price return on Day t

This part of the project  was dedicated towards creating a multiple linear regression model to explain price return on day t, by extracting at least three features from the token network at day t-1.

### Features:
1. How the "Most Important Users" behaved the previous day? Whether they sold more or bought more?
2. How many other unique users follow these "Most Important Users" behavior on the previous day?
3. What was the predominant trend on the previous day? Was the token price decreasing mostly or increasing?
4. What is the quantum of the increase or decrease in the price fluctuation on the previous day?


Find an intersection between the date ranges in both the data set and sort the dataframe in decreasing order of date.

We will be using the same token that we used for study in the previous sections.

```{r, warning = FALSE}
NEUTRAL_CONST = "N"
SELLING_DAY_CONST = "S"
BUYING_DAY_CONST = "B"
INCREASING_TREND_CONST = "I"
DECREASING_TREND_CONST ="D"
DATE_FORMAT_CONST = "%m/%d/%Y"


# Read the token graph.
binanceTokenGraph <- read.table("networkbnbTX.txt", header = FALSE)
names(binanceTokenGraph)<-c("seller","buyer","date","amount")

# Convert the timestamp to date format.
binanceTokenGraph$date <- format(as.Date(as.POSIXct(as.numeric(as.character(binanceTokenGraph$date)),origin="1970/01/01",tz="GMT"), tz="GMT"), DATE_FORMAT_CONST)

# Read the token prices
tokenPrices <- read.table("bnb.txt", header=TRUE, sep="\t")
names(tokenPrices) <- c("date", "open", "high", "low","close", "volume", "cap")
tokenPrices$volume <- as.numeric(gsub(",", "", tokenPrices$volume))


```


We will find the the total number of tokens sold and bought by each person on each day.

```{r, warning = FALSE}
tokenPrices <- tokenPrices[tokenPrices$date %in% unique(binanceTokenGraph$date), ]
binanceTokenGraph <- binanceTokenGraph[binanceTokenGraph$date %in% tokenPrices$date, ]

binanceTokenGraph <- binanceTokenGraph[rev(order(as.Date(binanceTokenGraph$date, format=DATE_FORMAT_CONST))),]
tokenPrices <- tokenPrices[rev(order(as.Date(tokenPrices$date, format=DATE_FORMAT_CONST))),]

tokensSoldPerUserPerDay <- aggregate(amount~date+seller, binanceTokenGraph, sum)

tokensBoughtPerUserPerDay <- aggregate(amount~date+buyer, binanceTokenGraph, sum)
```

Using this, we will find the the total number of tokens transacted by each person on each day and find the difference between the number of tokens sold and the number of tokens bought. If number of tokens sold > number of tokens bought, then the difference will be positive; denotes that there were more sales than purchases.

```{r, warning = FALSE}

tokensTransactedPerUserPerDay <- merge(tokensSoldPerUserPerDay, tokensBoughtPerUserPerDay, by.x=c("date", "seller"), by.y=c("date", "buyer"), all=TRUE)
tokensTransactedPerUserPerDay[is.na(tokensTransactedPerUserPerDay)] <- 0

names(tokensTransactedPerUserPerDay)<-c("date","user","tokensSold","tokensBought")
tokensTransactedPerUserPerDay$tokensTransacted <- (tokensTransactedPerUserPerDay$tokensSold - tokensTransactedPerUserPerDay$tokensBought)

tokenPrices <- tokenPrices[order(as.Date(tokenPrices$date, format=DATE_FORMAT_CONST)),]

#output column
for (priceIdx in 3:nrow(tokenPrices)-1) 
{ 
  tokenPrices$predicted_price_change[priceIdx] <- ( (tokenPrices$close[priceIdx] - tokenPrices$close[priceIdx-1])/ tokenPrices$close[priceIdx-1])
}

tokenPrices$predicted_price_change[is.na(tokenPrices$predicted_price_change)] <- 0

#predicted price
for( i in 3:nrow(tokenPrices)-1){
tokenPrices$predicted_price[i] = tokenPrices$close[i-1] + tokenPrices$predicted_price_change[i-1]
}

#sort by date
tokenPrices$profitability <- (tokenPrices$open - tokenPrices$close)

```
To find the difference in opening price and closing price on each day of the token, we use opening price instead of closing price from previous day because the difference between them is almost negligible.

The data frame containing the number of transactions per day per user is merged with the token prices (including the profitability column). This gives the number of days an user has made any transaction. These are the days when (s)he had made a bet on price change between that day and next day.
Find the number of days an user made accurate prediction on the price change

```{r, warning=FALSE}
mergedDF <- merge(tokensTransactedPerUserPerDay, tokenPrices, by.x=c("date"), by.y=c("date"))

noOfPredictions <- aggregate(tokensTransacted~user, mergedDF, length)

correctPredictions <- aggregate(tokensTransacted~user, mergedDF[((mergedDF$tokensTransacted > 0 & mergedDF$profitability > 0) | (mergedDF$tokensTransacted < 0 & mergedDF$profitability < 0)),], length)
  print(head(correctPredictions))

```
Combine the produced data frames to a vip dataframe and find their prediction accuracy, and get the VIP users with very high accuracy and a decent number of txns and the VIP users with many txns but decent accuracy.

```{r, warning=FALSE}
 vip <- merge(correctPredictions, noOfPredictions, by=c("user"), all=TRUE)
 rm(noOfPredictions)
 rm(correctPredictions)
 names(vip) <- c("user", "correctPredictions", "totalPredictions")
 vip[is.na(vip)] <- 0
 vip$predictionAccuracy <- (vip$correctPredictions/vip$totalPredictions)

 vipWithHighAccuracyOrFreqTxn <- vip[((vip$totalPredictions > 10 & vip$predictionAccuracy > 0.6) | (vip$totalPredictions > 100 & vip$predictionAccuracy > 0.49)),]

 vipWithFreqTxn <- vip[(vip$totalPredictions > 100 & vip$predictionAccuracy > 0.49),]
  print(head(vipWithHighAccuracyOrFreqTxn,10))
  print(head(vipWithFreqTxn))

```
Now we filter the transactions made by our chosen VIP users on each day and use an algorithm to find if the day was predominantly a selling day or a buying day or neither.
```{r, warning=FALSE}


 typeOfDayForVIPs <- aggregate(tokensTransacted~date, mergedDF[mergedDF$user %in% vipWithHighAccuracyOrFreqTxn$user,], sum)
 typeOfDayForVIPs[typeOfDayForVIPs$tokensTransacted > 0, "vip_day_type"] <- SELLING_DAY_CONST
 typeOfDayForVIPs[typeOfDayForVIPs$tokensTransacted < 0, "vip_day_type"] <- BUYING_DAY_CONST
 typeOfDayForVIPs[typeOfDayForVIPs$tokensTransacted == 0, "vip_day_type"] <- NEUTRAL_CONST
 colnames(typeOfDayForVIPs)[2] <- "vip_tokens_transacted"

 
 
 #prediciton for next day: depends on the type of day today
 for (priceIdx in 2:nrow(typeOfDayForVIPs)-1) 
 { 
   typeOfDayForVIPs$tokensTransacted[priceIdx] <- typeOfDayForVIPs$day_type[priceIdx-1]
 }
```
We're assuming the if the high and low are close to open price and close price, respectively, then it means throughout the day the token has been following a decreasing trend. Hence, we would predict the next day price to decrease.

 The quantum of decrease depends on the fluctuation on the previous day. On the other hand, if the high is closer to closing price, then the token has been following an increasing trend throughout the day. We would therefore predict an increase in closing price on the next day as well. The quantum of increase again depends on the fluctuation on the previous day.


Note: First day trend is simply going to be "neutral" since we do not have data about 0th day.
```{r, warning=FALSE}

trendFluctuationDF <- data.frame()

for (priceIdx in 2:nrow(tokenPrices)-1) { 
  # iterate over every day except the first day. Remember, last row is first day. So, nrow(tokenPrices)-1.
  today <- tokenPrices$date[priceIdx] # Today's date.
  previousDayPrice <- tokenPrices[priceIdx+1,] # Get the previous day price.
  diff_high_open <- abs(previousDayPrice$open - previousDayPrice$high) # Absolute difference between open and high.
  diff_low_open <- abs(previousDayPrice$open - previousDayPrice$low) # Absolute difference between open and low.
  diff_high_close <- abs(previousDayPrice$close - previousDayPrice$high) # Absolute difference between close and high.
  diff_low_close <- abs(previousDayPrice$close - previousDayPrice$low) # Absolute difference between close and low.
  diff_open_close <- previousDayPrice$close - previousDayPrice$open # If positive, it means upward trend.
  fluctuation <- abs(previousDayPrice$high - previousDayPrice$low) # Absolute difference between high and low
  
  row <- data.frame("date" = today, "trend" = NEUTRAL_CONST, "fluctuation" = 0.0, stringsAsFactors=FALSE)
  
  if (diff_high_open < diff_high_close & diff_low_close < diff_low_open) {
    # High is near open and low is near close - decreasing trend
    row[row$trend == NEUTRAL_CONST, "trend"] <- DECREASING_TREND_CONST
    row[1, "fluctuation"] <- fluctuation
  } else if (diff_high_close < diff_high_open & diff_low_open < diff_low_close) {
    # High is near close and low is near open - increasing trend
    row[row$trend == NEUTRAL_CONST, "trend"] <- INCREASING_TREND_CONST
    row[1, "fluctuation"] <- fluctuation
  } else if ((diff_high_close < diff_high_open & diff_low_close < diff_low_open)
             | (diff_high_open < diff_high_close & diff_low_open < diff_low_close)) {
    # High and low are near close or open -> see diff between close price and open price to determine trend.
    if (diff_open_close < 0) {
      # decreasing trend
      row[row$trend == NEUTRAL_CONST, "trend"] <- DECREASING_TREND_CONST
    } else {
      # increasing trend
      row[row$trend == NEUTRAL_CONST, "trend"] <- INCREASING_TREND_CONST
    }
    row[1, "fluctuation"] <- fluctuation
  }
  trendFluctuationDF <- rbind(row, trendFluctuationDF)
}
rm(row)
rm(previousDayPrice)
trendFluctuationDF <- rbind(data.frame("date" = tokenPrices[nrow(tokenPrices),"date"], "trend" = NEUTRAL_CONST, "fluctuation" = 0.0, stringsAsFactors=FALSE), trendFluctuationDF)

features <- merge(typeOfDayForVIPs, trendFluctuationDF, by.x=c("date"), by.y=c("date"), all=TRUE)
```
We will consider date and corresponding predicted price to extract rows where predicted price chnage is 0

```{r, warning=FALSE}
output_values <- tokenPrices[,c(1,8,9,10)]

features <- merge(features,output_values, by.x = "date" , by.y = "date")

sapply(features, class)

change <- subset(features, predicted_price_change == 0)
features[is.na(features$vip_tokens_transacted), "vip_tokens_transacted"] <- 0.0
features[features$vip_tokens_transacted == 0, "vip_day_type"] <- "N"
```
We will give numerical values to entries in features as follows.
```{r, warning=FALSE}
features[features$vip_day_type  == "B", "todaysPrediction"] <- 0
features[features$vip_day_type  == "S", "todaysPrediction"] <- 1
features[features$vip_day_type  == "N", "todaysPrediction"] <- -1


features[features$trend  == "I", "todays_trend"] <- 0
features[features$trend  == "D", "todays_trend"] <- 1
features[features$trend  == "N", "todays_trend"] <- -1
sapply(features, class)
```
To clean the data we remove values where fluctutaion is greater than 5, and remove values where profitability is less than -4

```{r, warning=FALSE, echo=FALSE}

features <- subset(features, fluctuation < 5)

features <- subset(features, profitability > -4)
features <- subset(features,  features$profitability < 3 )
```
####The regression Model:
```{r, warning=FALSE}
fit <- lm(predicted_price ~todays_trend + fluctuation+todaysPrediction, data = features)

summary(fit)

print(fit)


```

#5. Conclusion:
To summarize the whole process -  

Number of times user **sells** a token follows a **negative binomial distribution** with a mean of **`r meanOfDistribution`** and a variance of **`r varianceOfDistibution`**.

Number of times user **buys** a token follows a **negative binomial distribution** with a mean of **`r meanOfDistribution2`** and a variance of **`r varianceOfDistibution2`**.

We found that as **the number of transactions in a layer increases, the total amount transacted increases.** Since the total amount(in USD) is calculated using the closing price of the days when the transactions were made, we can conclude that the **number of transactions in each layer is positively correlated to the closing price**.

We also calculated the **correlation values** for a set of numbers and narrowed down to 31 as it gives the highest correlation value of **`r cor_value[2]`**.

From the plots and the estimated parameters, we found that **Weibull distribution** closely fits our data. Hence, the distribution of the number of unique tokens that users have invested in (apart from our primary token) follows a Weibull distribution.

Using the 4 features mentioned above, we can observe that the regression model predicts the price change on a particular day t using the characteristics of day t-1 with a model adequacy, denoted by r-squared, of 0.4773 or 47.73%.